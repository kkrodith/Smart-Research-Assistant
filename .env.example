# Ollama Configuration (completely free, unlimited, local)
# Ollama base URL (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Ollama model to use (options: llama2, mistral, codellama, neural-chat, etc.)
# Download models with: ollama pull <model-name>
OLLAMA_MODEL=llama2